# Learning-Resources

<meta name="google-site-verification" content="w16roLqKNFjcyRlizYF8_0WZ5kbhZAM3vdLULjJNqnY" />

A compilation of resources for keeping up with the latest trends in NLP.

> **Note:** This resource list is a work in progress. More papers and topics will be added regularly. Contributions and suggestions are welcome!

## Some Fundamental Transformers

1. [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
2. [GPT1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
3. [GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
4. [T5](https://arxiv.org/abs/1910.10683)
5. [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237)
6. [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692)
7. [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)
8. [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)
9. [ROFORMER](https://arxiv.org/pdf/2104.09864)
10. [Language Models are Few-Shot Learners - GPT3 paper](https://arxiv.org/abs/2005.14165)

## Fundamental LLM & Transformer Papers/Blogs

1. [Attention is all you need](https://arxiv.org/pdf/1706.03762)
2. [Memory Is All You Need](https://arxiv.org/pdf/2406.08413)
3. [Byte-pair Encoding](https://arxiv.org/pdf/1508.07909)
4. [The Illustrated Transformer Blog](https://jalammar.github.io/illustrated-transformer/)
5. [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer - MoE paper for LMs](https://arxiv.org/abs/1701.06538)
6. [Fast Transformer Decoding: One Write-Head is All You Need - Multi-Query Attention (MQA) Paper](https://arxiv.org/pdf/1911.02150)
7. [GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints - Grouped Query Attention Paper](https://arxiv.org/pdf/2305.13245)

## Reinforcement Learning for LLMs

1. [Basics of RL - OpenAI](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)
2. [Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism ](https://arxiv.org/abs/2305.18438)
3. [InstructGPT ](https://arxiv.org/abs/2203.02155)
4. [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155)
5. [Deep Reinforcement Learning from Human Preferences](https://arxiv.org/pdf/1706.03741)

DPO:

1. [DPO paper](https://arxiv.org/pdf/2305.18290)
2. [Blog - Math behind DPO](https://www.tylerromero.com/posts/2024-04-dpo/)

PPO:

1. [Proximal Policy Optimization Algorithms](https://arxiv.org/pdf/1707.06347)
2. [PPO Docs OpenAI](https://spinningup.openai.com/en/latest/algorithms/ppo.html)
3. [Understanding PPO from First Principles Blog](https://mlwithouttears.com/2025/11/17/understanding-ppo-from-first-principles/)

GRPO: 

1. [DeepSeekMath](https://arxiv.org/abs/2402.03300)
2. [Blog - GRPO Explained ](https://aipapersacademy.com/deepseekmath-grpo/)
3. [DeepSeek-R1]( https://arxiv.org/pdf/2501.12948)

## Mechanistic Interpretability

1. [Basic Mech Interp Essay](https://www.transformer-circuits.pub/2022/mech-interp-essay)
2. [Toy Neural Nets with low dimensional inputs ](https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
3. [Mechanistic Interpretability for AI Safety Review]( https://arxiv.org/abs/2404.14082)
4. [A Mathematical Framework for Transformer Circuits]( https://transformer-circuits.pub/2021/framework/index.html)
5. [Circuit Tracing: Revealing Computational Graphs in Language Models ](https://transformer-circuits.pub/2025/attribution-graphs/methods.html#evaluating-model)

## Scaling Laws

1. [Scaling Laws for Neural Language Models ](https://arxiv.org/pdf/2001.08361)
2. [Scaling Laws for Autoregressive Generative Modeling ](https://arxiv.org/pdf/2010.14701)
3. [Sacling Laws of Synthetic Data for Lnguage Models ](https://arxiv.org/pdf/2503.19551)
4. [Scaling Laws for Transfer](https://arxiv.org/abs/2102.01293)
5. [Unified Scaling Laws for Routed Language Models - Scaling laws for MoEs](https://arxiv.org/abs/2202.01169)

## MLSys

1. [Mixed Precision Training](https://arxiv.org/pdf/1710.03740)
2. [Matrix multiplication - Nvidia Blog](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html)
3. [Understanding GPU Performance - Nvidia Blog](https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html#gpu-arch__fig2)
4. [How to Train Really Large Models on Many GPUs? - Blog](https://lilianweng.github.io/posts/2021-09-25-train-large/)
5. [Efficiently Scaling Transformer Inference](https://arxiv.org/pdf/2211.05102)
6. [DistServe: Disaggregating Prefill and Decoding for Goodput-optimized
Large Language Model Serving](https://arxiv.org/pdf/2401.09670)
7. [SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills](https://arxiv.org/pdf/2308.16369)
